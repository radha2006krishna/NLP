{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/rW7CFFOFP3SAtyjGTj86",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radha2006krishna/NLP/blob/main/Assignment_5_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "b9iZZKLn524H"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load data**"
      ],
      "metadata": {
        "id": "5fateYF0ByKD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a=pd.read_csv(\"/content/Tweets.csv\")\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 929
        },
        "id": "eM2YthNo6YTv",
        "outputId": "2e3d1df0-08eb-4b36-d637-5d2fb2cd01a9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 tweet_id airline_sentiment  airline_sentiment_confidence  \\\n",
              "0      570306133677760513           neutral                        1.0000   \n",
              "1      570301130888122368          positive                        0.3486   \n",
              "2      570301083672813571           neutral                        0.6837   \n",
              "3      570301031407624196          negative                        1.0000   \n",
              "4      570300817074462722          negative                        1.0000   \n",
              "...                   ...               ...                           ...   \n",
              "14635  569587686496825344          positive                        0.3487   \n",
              "14636  569587371693355008          negative                        1.0000   \n",
              "14637  569587242672398336           neutral                        1.0000   \n",
              "14638  569587188687634433          negative                        1.0000   \n",
              "14639  569587140490866689           neutral                        0.6771   \n",
              "\n",
              "               negativereason  negativereason_confidence         airline  \\\n",
              "0                         NaN                        NaN  Virgin America   \n",
              "1                         NaN                     0.0000  Virgin America   \n",
              "2                         NaN                        NaN  Virgin America   \n",
              "3                  Bad Flight                     0.7033  Virgin America   \n",
              "4                  Can't Tell                     1.0000  Virgin America   \n",
              "...                       ...                        ...             ...   \n",
              "14635                     NaN                     0.0000        American   \n",
              "14636  Customer Service Issue                     1.0000        American   \n",
              "14637                     NaN                        NaN        American   \n",
              "14638  Customer Service Issue                     0.6659        American   \n",
              "14639                     NaN                     0.0000        American   \n",
              "\n",
              "      airline_sentiment_gold             name negativereason_gold  \\\n",
              "0                        NaN          cairdin                 NaN   \n",
              "1                        NaN         jnardino                 NaN   \n",
              "2                        NaN       yvonnalynn                 NaN   \n",
              "3                        NaN         jnardino                 NaN   \n",
              "4                        NaN         jnardino                 NaN   \n",
              "...                      ...              ...                 ...   \n",
              "14635                    NaN  KristenReenders                 NaN   \n",
              "14636                    NaN         itsropes                 NaN   \n",
              "14637                    NaN         sanyabun                 NaN   \n",
              "14638                    NaN       SraJackson                 NaN   \n",
              "14639                    NaN        daviddtwu                 NaN   \n",
              "\n",
              "       retweet_count                                               text  \\\n",
              "0                  0                @VirginAmerica What @dhepburn said.   \n",
              "1                  0  @VirginAmerica plus you've added commercials t...   \n",
              "2                  0  @VirginAmerica I didn't today... Must mean I n...   \n",
              "3                  0  @VirginAmerica it's really aggressive to blast...   \n",
              "4                  0  @VirginAmerica and it's a really big bad thing...   \n",
              "...              ...                                                ...   \n",
              "14635              0  @AmericanAir thank you we got on a different f...   \n",
              "14636              0  @AmericanAir leaving over 20 minutes Late Flig...   \n",
              "14637              0  @AmericanAir Please bring American Airlines to...   \n",
              "14638              0  @AmericanAir you have my money, you change my ...   \n",
              "14639              0  @AmericanAir we have 8 ppl so we need 2 know h...   \n",
              "\n",
              "      tweet_coord              tweet_created tweet_location  \\\n",
              "0             NaN  2015-02-24 11:35:52 -0800            NaN   \n",
              "1             NaN  2015-02-24 11:15:59 -0800            NaN   \n",
              "2             NaN  2015-02-24 11:15:48 -0800      Lets Play   \n",
              "3             NaN  2015-02-24 11:15:36 -0800            NaN   \n",
              "4             NaN  2015-02-24 11:14:45 -0800            NaN   \n",
              "...           ...                        ...            ...   \n",
              "14635         NaN  2015-02-22 12:01:01 -0800            NaN   \n",
              "14636         NaN  2015-02-22 11:59:46 -0800          Texas   \n",
              "14637         NaN  2015-02-22 11:59:15 -0800  Nigeria,lagos   \n",
              "14638         NaN  2015-02-22 11:59:02 -0800     New Jersey   \n",
              "14639         NaN  2015-02-22 11:58:51 -0800     dallas, TX   \n",
              "\n",
              "                    user_timezone  \n",
              "0      Eastern Time (US & Canada)  \n",
              "1      Pacific Time (US & Canada)  \n",
              "2      Central Time (US & Canada)  \n",
              "3      Pacific Time (US & Canada)  \n",
              "4      Pacific Time (US & Canada)  \n",
              "...                           ...  \n",
              "14635                         NaN  \n",
              "14636                         NaN  \n",
              "14637                         NaN  \n",
              "14638  Eastern Time (US & Canada)  \n",
              "14639                         NaN  \n",
              "\n",
              "[14640 rows x 15 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-44f00e2e-6a6c-44a9-8c43-1bef878577ff\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>airline_sentiment_confidence</th>\n",
              "      <th>negativereason</th>\n",
              "      <th>negativereason_confidence</th>\n",
              "      <th>airline</th>\n",
              "      <th>airline_sentiment_gold</th>\n",
              "      <th>name</th>\n",
              "      <th>negativereason_gold</th>\n",
              "      <th>retweet_count</th>\n",
              "      <th>text</th>\n",
              "      <th>tweet_coord</th>\n",
              "      <th>tweet_created</th>\n",
              "      <th>tweet_location</th>\n",
              "      <th>user_timezone</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>570306133677760513</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>cairdin</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica What @dhepburn said.</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:35:52 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>570301130888122368</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3486</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:59 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>570301083672813571</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6837</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>yvonnalynn</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica I didn't today... Must mean I n...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:48 -0800</td>\n",
              "      <td>Lets Play</td>\n",
              "      <td>Central Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>570301031407624196</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Bad Flight</td>\n",
              "      <td>0.7033</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:15:36 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>570300817074462722</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Can't Tell</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Virgin America</td>\n",
              "      <td>NaN</td>\n",
              "      <td>jnardino</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-24 11:14:45 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Pacific Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14635</th>\n",
              "      <td>569587686496825344</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.3487</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>KristenReenders</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir thank you we got on a different f...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 12:01:01 -0800</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14636</th>\n",
              "      <td>569587371693355008</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Customer Service Issue</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>itsropes</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir leaving over 20 minutes Late Flig...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 11:59:46 -0800</td>\n",
              "      <td>Texas</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14637</th>\n",
              "      <td>569587242672398336</td>\n",
              "      <td>neutral</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>sanyabun</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir Please bring American Airlines to...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 11:59:15 -0800</td>\n",
              "      <td>Nigeria,lagos</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14638</th>\n",
              "      <td>569587188687634433</td>\n",
              "      <td>negative</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>Customer Service Issue</td>\n",
              "      <td>0.6659</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>SraJackson</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir you have my money, you change my ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 11:59:02 -0800</td>\n",
              "      <td>New Jersey</td>\n",
              "      <td>Eastern Time (US &amp; Canada)</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14639</th>\n",
              "      <td>569587140490866689</td>\n",
              "      <td>neutral</td>\n",
              "      <td>0.6771</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>American</td>\n",
              "      <td>NaN</td>\n",
              "      <td>daviddtwu</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0</td>\n",
              "      <td>@AmericanAir we have 8 ppl so we need 2 know h...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2015-02-22 11:58:51 -0800</td>\n",
              "      <td>dallas, TX</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>14640 rows Ã— 15 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-44f00e2e-6a6c-44a9-8c43-1bef878577ff')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-44f00e2e-6a6c-44a9-8c43-1bef878577ff button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-44f00e2e-6a6c-44a9-8c43-1bef878577ff');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_c77b99b1-e01a-4bb2-9fb2-d26eba971a24\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('a')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_c77b99b1-e01a-4bb2-9fb2-d26eba971a24 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('a');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "a",
              "summary": "{\n  \"name\": \"a\",\n  \"rows\": 14640,\n  \"fields\": [\n    {\n      \"column\": \"tweet_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 779111158481836,\n        \"min\": 567588278875213824,\n        \"max\": 570310600460525568,\n        \"num_unique_values\": 14485,\n        \"samples\": [\n          567917894144770049,\n          567813976492417024,\n          569243676594941953\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"airline_sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"neutral\",\n          \"positive\",\n          \"negative\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"airline_sentiment_confidence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.1628299590986659,\n        \"min\": 0.335,\n        \"max\": 1.0,\n        \"num_unique_values\": 1023,\n        \"samples\": [\n          0.6723,\n          0.3551,\n          0.6498\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"negativereason\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"Damaged Luggage\",\n          \"Can't Tell\",\n          \"Lost Luggage\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"negativereason_confidence\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.3304397596377413,\n        \"min\": 0.0,\n        \"max\": 1.0,\n        \"num_unique_values\": 1410,\n        \"samples\": [\n          0.6677,\n          0.6622,\n          0.6905\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"airline\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          \"Virgin America\",\n          \"United\",\n          \"American\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"airline_sentiment_gold\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"negative\",\n          \"neutral\",\n          \"positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"name\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 7701,\n        \"samples\": [\n          \"smckenna719\",\n          \"thisAnneM\",\n          \"jmspool\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"negativereason_gold\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 13,\n        \"samples\": [\n          \"Customer Service Issue\\nLost Luggage\",\n          \"Late Flight\\nCancelled Flight\",\n          \"Late Flight\\nFlight Attendant Complaints\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"retweet_count\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 44,\n        \"num_unique_values\": 18,\n        \"samples\": [\n          0,\n          1,\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 14427,\n        \"samples\": [\n          \"@JetBlue so technically I could drive to JFK now and put in. Request for tomorrow's flight?\",\n          \"@united why I won't check my carry on. Watched a handler throw this bag -- miss the conveyer belt -- sat there 10 min http://t.co/lyoocx5mSH\",\n          \"@SouthwestAir you guys are so clever \\ud83d\\ude03 http://t.co/qn5odUGFqK\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tweet_coord\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 832,\n        \"samples\": [\n          \"[40.04915451, -75.10364317]\",\n          \"[32.97609561, -96.53349238]\",\n          \"[26.37852293, -81.78472152]\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tweet_created\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"num_unique_values\": 14247,\n        \"samples\": [\n          \"2015-02-23 07:40:55 -0800\",\n          \"2015-02-21 16:20:09 -0800\",\n          \"2015-02-21 21:33:21 -0800\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tweet_location\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 3081,\n        \"samples\": [\n          \"Oakland, California\",\n          \"Beverly Hills, CA\",\n          \"Austin, TX/NY, NY\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"user_timezone\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 85,\n        \"samples\": [\n          \"Helsinki\",\n          \"Eastern Time (US & Canada)\",\n          \"America/Detroit\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Extract Text in Tweet**"
      ],
      "metadata": {
        "id": "419-kG1yB7HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweets=a['text']\n",
        "print(\"Extracted tweets (first 5):\")\n",
        "for i, tweet in enumerate(tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "jCQTw9jA6jq5",
        "outputId": "56a6f5cd-bcf8-4830-a368-28ea6dcb46d2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted tweets (first 5):\n",
            "1. @VirginAmerica What @dhepburn said.\n",
            "2. @VirginAmerica plus you've added commercials to the experience... tacky.\n",
            "3. @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
            "4. @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
            "5. @VirginAmerica and it's a really big bad thing about it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Visulaize Hashtags**"
      ],
      "metadata": {
        "id": "1uJ-JQ2fCAAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def extract_actual_hashtags(text):\n",
        "    return re.findall(r'#\\w+', text)\n",
        "extracted_hashtags = tweets.apply(extract_actual_hashtags)\n",
        "print(\"Corrected extracted hashtags from preprocessed tweets (first 5):\")\n",
        "for i, hashtags in enumerate(extracted_hashtags.head()):\n",
        "    print(f\"{i+1}. {hashtags}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bG4cH3EU7EdB",
        "outputId": "f59f799f-dcbe-49a9-f729-2748b6b5ea69"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Corrected extracted hashtags from preprocessed tweets (first 5):\n",
            "1. []\n",
            "2. []\n",
            "3. []\n",
            "4. []\n",
            "5. []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "all_hashtags = [hashtag for sublist in extracted_hashtags for hashtag in sublist]\n",
        "hashtag_frequencies = Counter(all_hashtags)\n",
        "print(\"Top 10 most common hashtags:\")\n",
        "for hashtag, count in hashtag_frequencies.most_common(10):\n",
        "    print(f\"- {hashtag}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YqSzEoI-9d_y",
        "outputId": "40f2e673-7744-42da-9844-a13f2878dafd"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 10 most common hashtags:\n",
            "- #DestinationDragons: 75\n",
            "- #fail: 57\n",
            "- #jetblue: 35\n",
            "- #UnitedAirlines: 35\n",
            "- #customerservice: 34\n",
            "- #usairwaysfail: 26\n",
            "- #AmericanAirlines: 24\n",
            "- #disappointed: 22\n",
            "- #avgeek: 19\n",
            "- #badservice: 19\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Clean Text Data**"
      ],
      "metadata": {
        "id": "QAfNamzICG84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text.strip()\n",
        "cleaned_tweets = tweets.apply(clean_text)\n",
        "print(\"Original tweets (first 5):\")\n",
        "for i, tweet in enumerate(tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")\n",
        "print(\"\\nCleaned tweets (first 5):\")\n",
        "for i, tweet in enumerate(cleaned_tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "MkT5lHb5-G_I",
        "outputId": "5f8cfb2b-a04b-41ca-f8eb-9528bb55aed5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original tweets (first 5):\n",
            "1. @VirginAmerica What @dhepburn said.\n",
            "2. @VirginAmerica plus you've added commercials to the experience... tacky.\n",
            "3. @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
            "4. @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
            "5. @VirginAmerica and it's a really big bad thing about it\n",
            "\n",
            "Cleaned tweets (first 5):\n",
            "1. what  said\n",
            "2. plus youve added commercials to the experience tacky\n",
            "3. i didnt today must mean i need to take another trip\n",
            "4. its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse\n",
            "5. and its a really big bad thing about it\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Word Tokenization**"
      ],
      "metadata": {
        "id": "6URbsSYSCMNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "tokenized_tweets = cleaned_tweets.apply(tokenize_text)\n",
        "print(\"Cleaned tweets (first 5):\")\n",
        "for i, tweet in enumerate(cleaned_tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")\n",
        "print(\"\\nTokenized tweets (first 5):\")\n",
        "for i, tokens in enumerate(tokenized_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9NOBrjbZ-cp_",
        "outputId": "64419a27-b488-4b3f-d0b6-5bcef835e27a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned tweets (first 5):\n",
            "1. what  said\n",
            "2. plus youve added commercials to the experience tacky\n",
            "3. i didnt today must mean i need to take another trip\n",
            "4. its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse\n",
            "5. and its a really big bad thing about it\n",
            "\n",
            "Tokenized tweets (first 5):\n",
            "1. ['what', 'said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'to', 'the', 'experience', 'tacky']\n",
            "3. ['i', 'didnt', 'today', 'must', 'mean', 'i', 'need', 'to', 'take', 'another', 'trip']\n",
            "4. ['its', 'really', 'aggressive', 'to', 'blast', 'obnoxious', 'entertainment', 'in', 'your', 'guests', 'faces', 'amp', 'they', 'have', 'little', 'recourse']\n",
            "5. ['and', 'its', 'a', 'really', 'big', 'bad', 'thing', 'about', 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Stop word removal**"
      ],
      "metadata": {
        "id": "3zHi0AE_CSiM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "filtered_tweets = tokenized_tweets.apply(remove_stopwords)\n",
        "print(\"Tokenized tweets (first 5):\")\n",
        "for i, tokens in enumerate(tokenized_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")\n",
        "print(\"\\nFiltered tweets (first 5) (stopwords removed):\")\n",
        "for i, tokens in enumerate(filtered_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f5C0onSM_AmY",
        "outputId": "e6698e37-b9b9-4ff5-9b41-d8b1ece10b45"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized tweets (first 5):\n",
            "1. ['what', 'said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'to', 'the', 'experience', 'tacky']\n",
            "3. ['i', 'didnt', 'today', 'must', 'mean', 'i', 'need', 'to', 'take', 'another', 'trip']\n",
            "4. ['its', 'really', 'aggressive', 'to', 'blast', 'obnoxious', 'entertainment', 'in', 'your', 'guests', 'faces', 'amp', 'they', 'have', 'little', 'recourse']\n",
            "5. ['and', 'its', 'a', 'really', 'big', 'bad', 'thing', 'about', 'it']\n",
            "\n",
            "Filtered tweets (first 5) (stopwords removed):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guests', 'faces', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Lemmatization**"
      ],
      "metadata": {
        "id": "E2v05FlaCW6r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "lemmatized_tweets = filtered_tweets.apply(lemmatize_tokens)\n",
        "print(\"Filtered tweets (first 5):\")\n",
        "for i, tokens in enumerate(filtered_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")\n",
        "print(\"\\nLemmatized tweets (first 5):\")\n",
        "for i, tokens in enumerate(lemmatized_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TszNLiqI_QEO",
        "outputId": "6c6cf94d-5ec7-409e-dd69-5ffc32f8f84b"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered tweets (first 5):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guests', 'faces', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n",
            "\n",
            "Lemmatized tweets (first 5):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercial', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guest', 'face', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered tweets (first 5):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guests', 'faces', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n",
            "\n",
            "Lemmatized tweets (first 5):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercial', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guest', 'face', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Rejoining (Processed Tweet and Corresponding Label)**"
      ],
      "metadata": {
        "id": "bAzKjQS6CbCd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def join_tokens(tokens):\n",
        "    return ' '.join(tokens)\n",
        "preprocessed_tweets = lemmatized_tweets.apply(join_tokens)\n",
        "print(\"Preprocessed tweets (first 5):\")\n",
        "for i, tweet in enumerate(preprocessed_tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "-ZzXLaVDAA0v",
        "outputId": "1daef9a5-5eb8-4a0b-e3dc-2cfa768cd915"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessed tweets (first 5):\n",
            "1. said\n",
            "2. plus youve added commercial experience tacky\n",
            "3. didnt today must mean need take another trip\n",
            "4. really aggressive blast obnoxious entertainment guest face amp little recourse\n",
            "5. really big bad thing\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "72c4d3f9",
        "outputId": "a9544b48-c78d-4b86-9cd8-397c4fcc5f3e"
      },
      "source": [
        "labeled_data=pd.DataFrame({'text':preprocessed_tweets,'sentiment':a['airline_sentiment']})\n",
        "print(\"Created labeled_data DataFrame. Displaying first 5 rows:\")\n",
        "print(labeled_data.head())"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created labeled_data DataFrame. Displaying first 5 rows:\n",
            "                                                text sentiment\n",
            "0                                               said   neutral\n",
            "1       plus youve added commercial experience tacky  positive\n",
            "2       didnt today must mean need take another trip   neutral\n",
            "3  really aggressive blast obnoxious entertainmen...  negative\n",
            "4                               really big bad thing  negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **pipeline**"
      ],
      "metadata": {
        "id": "BoFfNuGoJrVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "a=pd.read_csv(\"/content/Tweets.csv\")\n",
        "a\n",
        "\n",
        "tweets=a['text']\n",
        "print(\"Extracted tweets (first 5):\")\n",
        "for i, tweet in enumerate(tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")\n",
        "\n",
        "import re\n",
        "def extract_actual_hashtags(text):\n",
        "    return re.findall(r'#\\w+', text)\n",
        "extracted_hashtags = tweets.apply(extract_actual_hashtags)\n",
        "print(\"Corrected extracted hashtags from preprocessed tweets (first 5):\")\n",
        "for i, hashtags in enumerate(extracted_hashtags.head()):\n",
        "    print(f\"{i+1}. {hashtags}\")\n",
        "\n",
        "from collections import Counter\n",
        "all_hashtags = [hashtag for sublist in extracted_hashtags for hashtag in sublist]\n",
        "hashtag_frequencies = Counter(all_hashtags)\n",
        "print(\"Top 10 most common hashtags:\")\n",
        "for hashtag, count in hashtag_frequencies.most_common(10):\n",
        "    print(f\"- {hashtag}: {count}\")\n",
        "\n",
        "import re\n",
        "def clean_text(text):\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    text = re.sub(r'http\\S+|www\\S+', '', text)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    text = text.lower()\n",
        "    return text.strip()\n",
        "cleaned_tweets = tweets.apply(clean_text)\n",
        "print(\"Original tweets (first 5):\")\n",
        "for i, tweet in enumerate(tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")\n",
        "print(\"\\nCleaned tweets (first 5):\")\n",
        "for i, tweet in enumerate(cleaned_tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "def tokenize_text(text):\n",
        "    return word_tokenize(text)\n",
        "tokenized_tweets = cleaned_tweets.apply(tokenize_text)\n",
        "print(\"Cleaned tweets (first 5):\")\n",
        "for i, tweet in enumerate(cleaned_tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")\n",
        "print(\"\\nTokenized tweets (first 5):\")\n",
        "for i, tokens in enumerate(tokenized_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stop_words]\n",
        "filtered_tweets = tokenized_tweets.apply(remove_stopwords)\n",
        "print(\"Tokenized tweets (first 5):\")\n",
        "for i, tokens in enumerate(tokenized_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")\n",
        "print(\"\\nFiltered tweets (first 5) (stopwords removed):\")\n",
        "for i, tokens in enumerate(filtered_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def lemmatize_tokens(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "lemmatized_tweets = filtered_tweets.apply(lemmatize_tokens)\n",
        "print(\"Filtered tweets (first 5):\")\n",
        "for i, tokens in enumerate(filtered_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")\n",
        "print(\"\\nLemmatized tweets (first 5):\")\n",
        "for i, tokens in enumerate(lemmatized_tweets.head()):\n",
        "    print(f\"{i+1}. {tokens}\")\n",
        "\n",
        "def join_tokens(tokens):\n",
        "    return ' '.join(tokens)\n",
        "preprocessed_tweets = lemmatized_tweets.apply(join_tokens)\n",
        "print(\"Preprocessed tweets (first 5):\")\n",
        "for i, tweet in enumerate(preprocessed_tweets.head()):\n",
        "    print(f\"{i+1}. {tweet}\")\n",
        "\n",
        "labeled_data=pd.DataFrame({'text':preprocessed_tweets,'sentiment':a['airline_sentiment']})\n",
        "print(\"Created labeled_data DataFrame. Displaying first 5 rows:\")\n",
        "print(labeled_data.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbLJiaM1IwM5",
        "outputId": "5e91b3b6-d2b7-4143-a20a-e08c4e431418"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted tweets (first 5):\n",
            "1. @VirginAmerica What @dhepburn said.\n",
            "2. @VirginAmerica plus you've added commercials to the experience... tacky.\n",
            "3. @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
            "4. @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
            "5. @VirginAmerica and it's a really big bad thing about it\n",
            "Corrected extracted hashtags from preprocessed tweets (first 5):\n",
            "1. []\n",
            "2. []\n",
            "3. []\n",
            "4. []\n",
            "5. []\n",
            "Top 10 most common hashtags:\n",
            "- #DestinationDragons: 75\n",
            "- #fail: 57\n",
            "- #jetblue: 35\n",
            "- #UnitedAirlines: 35\n",
            "- #customerservice: 34\n",
            "- #usairwaysfail: 26\n",
            "- #AmericanAirlines: 24\n",
            "- #disappointed: 22\n",
            "- #avgeek: 19\n",
            "- #badservice: 19\n",
            "Original tweets (first 5):\n",
            "1. @VirginAmerica What @dhepburn said.\n",
            "2. @VirginAmerica plus you've added commercials to the experience... tacky.\n",
            "3. @VirginAmerica I didn't today... Must mean I need to take another trip!\n",
            "4. @VirginAmerica it's really aggressive to blast obnoxious \"entertainment\" in your guests' faces &amp; they have little recourse\n",
            "5. @VirginAmerica and it's a really big bad thing about it\n",
            "\n",
            "Cleaned tweets (first 5):\n",
            "1. what  said\n",
            "2. plus youve added commercials to the experience tacky\n",
            "3. i didnt today must mean i need to take another trip\n",
            "4. its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse\n",
            "5. and its a really big bad thing about it\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned tweets (first 5):\n",
            "1. what  said\n",
            "2. plus youve added commercials to the experience tacky\n",
            "3. i didnt today must mean i need to take another trip\n",
            "4. its really aggressive to blast obnoxious entertainment in your guests faces amp they have little recourse\n",
            "5. and its a really big bad thing about it\n",
            "\n",
            "Tokenized tweets (first 5):\n",
            "1. ['what', 'said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'to', 'the', 'experience', 'tacky']\n",
            "3. ['i', 'didnt', 'today', 'must', 'mean', 'i', 'need', 'to', 'take', 'another', 'trip']\n",
            "4. ['its', 'really', 'aggressive', 'to', 'blast', 'obnoxious', 'entertainment', 'in', 'your', 'guests', 'faces', 'amp', 'they', 'have', 'little', 'recourse']\n",
            "5. ['and', 'its', 'a', 'really', 'big', 'bad', 'thing', 'about', 'it']\n",
            "Tokenized tweets (first 5):\n",
            "1. ['what', 'said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'to', 'the', 'experience', 'tacky']\n",
            "3. ['i', 'didnt', 'today', 'must', 'mean', 'i', 'need', 'to', 'take', 'another', 'trip']\n",
            "4. ['its', 'really', 'aggressive', 'to', 'blast', 'obnoxious', 'entertainment', 'in', 'your', 'guests', 'faces', 'amp', 'they', 'have', 'little', 'recourse']\n",
            "5. ['and', 'its', 'a', 'really', 'big', 'bad', 'thing', 'about', 'it']\n",
            "\n",
            "Filtered tweets (first 5) (stopwords removed):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guests', 'faces', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Filtered tweets (first 5):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercials', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guests', 'faces', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n",
            "\n",
            "Lemmatized tweets (first 5):\n",
            "1. ['said']\n",
            "2. ['plus', 'youve', 'added', 'commercial', 'experience', 'tacky']\n",
            "3. ['didnt', 'today', 'must', 'mean', 'need', 'take', 'another', 'trip']\n",
            "4. ['really', 'aggressive', 'blast', 'obnoxious', 'entertainment', 'guest', 'face', 'amp', 'little', 'recourse']\n",
            "5. ['really', 'big', 'bad', 'thing']\n",
            "Preprocessed tweets (first 5):\n",
            "1. said\n",
            "2. plus youve added commercial experience tacky\n",
            "3. didnt today must mean need take another trip\n",
            "4. really aggressive blast obnoxious entertainment guest face amp little recourse\n",
            "5. really big bad thing\n",
            "Created labeled_data DataFrame. Displaying first 5 rows:\n",
            "                                                text sentiment\n",
            "0                                               said   neutral\n",
            "1       plus youve added commercial experience tacky  positive\n",
            "2       didnt today must mean need take another trip   neutral\n",
            "3  really aggressive blast obnoxious entertainmen...  negative\n",
            "4                               really big bad thing  negative\n"
          ]
        }
      ]
    }
  ]
}